{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue' size=5>\n",
    "NN non-linear with linear reg final layer+sampler\n",
    "</font>\n",
    "\n",
    "--------\n",
    "\n",
    "- Err var = 0.25\n",
    "- WITH beta sampling while generating rewards\n",
    "- A small coefficient for X1X2 interaction\n",
    "- small correlation btwn X1 and X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pickle\n",
    "import sklearn\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import yaml\n",
    "import random\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',200)\n",
    "pd.set_option('display.width',500)\n",
    "sns.set()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "from scipy.stats import invgamma\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test create offline data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assume mean and covariance matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.1  , 1.   , 0.01 ],\n",
       "       [1.25 , 0.3  , 0.5  , 0.015],\n",
       "       [1.5  , 0.5  , 0.1  , 0.01 ],\n",
       "       [1.75 , 0.3  , 0.5  , 0.015],\n",
       "       [2.   , 0.1  , 1.   , 0.01 ]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu_vec = np.array([[1   , 0.1 ,  1 ,0.01  ],\n",
    "                   [1.25, 0.3 ,0.5 ,0.015 ],\n",
    "                   [1.5 , 0.5 ,0.1 ,0.01  ],\n",
    "                   [1.75, 0.3 ,0.5 ,0.015 ],\n",
    "                   [2.0 , 0.1 ,  1 ,0.01  ] ])\n",
    "# mu_vec = np.array([[1   , 0.1 ,  1 ,0.0 ],\n",
    "#                    [1.25, 0.3 ,0.5 ,0.0 ],\n",
    "#                    [1.5 , 0.5 ,0.1 ,0.0 ],\n",
    "#                    [1.75, 0.3 ,0.5 ,0.0 ],\n",
    "#                    [2.0 , 0.1 ,  1 ,0.0 ] ])\n",
    "\n",
    "mu_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.16944353, 0.02443279, 0.02225808, 0.        ],\n",
       "        [0.02443279, 0.16564317, 0.02068194, 0.        ],\n",
       "        [0.02225808, 0.02068194, 0.16085486, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.005     ]]),\n",
       " array([[0.16093093, 0.02212785, 0.02051755, 0.        ],\n",
       "        [0.02212785, 0.16535366, 0.0216939 , 0.        ],\n",
       "        [0.02051755, 0.0216939 , 0.16199029, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.005     ]]),\n",
       " array([[0.16363424, 0.01989482, 0.02782333, 0.        ],\n",
       "        [0.01989482, 0.16154533, 0.02798823, 0.        ],\n",
       "        [0.02782333, 0.02798823, 0.17400999, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.005     ]]),\n",
       " array([[0.16688277, 0.02488543, 0.02542759, 0.        ],\n",
       "        [0.02488543, 0.16537696, 0.02370056, 0.        ],\n",
       "        [0.02542759, 0.02370056, 0.16508331, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.005     ]]),\n",
       " array([[0.16482668, 0.02970371, 0.02577395, 0.        ],\n",
       "        [0.02970371, 0.17214919, 0.03152725, 0.        ],\n",
       "        [0.02577395, 0.03152725, 0.16943732, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.005     ]])]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_arms   = 5\n",
    "matrixSize = 4\n",
    "parmCovMat = []\n",
    "\n",
    "for i in range(num_arms):\n",
    "    _tempA  = np.random.rand(1000, matrixSize)\n",
    "    pCovMat = np.dot(_tempA.transpose(), _tempA) #To get a positive definite matrix\n",
    "    pCovMat = pCovMat/2000 #To reduce the scale of variances and covariances. \n",
    "    pCovMat +=np.ones((matrixSize, matrixSize)) * np.array([[0.0, -0.1,-0.1, 0.0],\n",
    "                                                            [-0.1, 0.0,-0.1, 0.0],\n",
    "                                                            [-0.1,-0.1, 0.0, 0.0],\n",
    "                                                            [ 0.0, 0.0, 0.0, 0.0]]) # To increase the variances [as in ridge regression] i.e. implying lower covariances between parameters\n",
    "    pCovMat[matrixSize-1,:] = 0.0\n",
    "    pCovMat[:,matrixSize-1] = 0.0\n",
    "    pCovMat[matrixSize-1,matrixSize-1] = 0.005\n",
    "    parmCovMat.append(pCovMat)\n",
    "\n",
    "#     pCovMat +=np.ones((matrixSize, matrixSize)) * np.array([[0.2, -0.05, 0.05],\n",
    "#                                                             [-0.05, 0.5, 0.0],\n",
    "#                                                             [-0.05, 0.0, 0.5]]) # To increase the variances [as in ridge regression] i.e. implying lower covariances between parameters\n",
    "parmCovMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a batch of data. \n",
    "\n",
    "## Instead of fixed table, you could also wrap new data generation within a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mumat  = np.array([5, 5])\n",
    "X_covmat = np.array([[5, 1],\n",
    "                      [1 ,5]]) #2 variables\n",
    "\n",
    "datarows = 10000\n",
    "X_matrix     = np.random.multivariate_normal(mean= X_mumat, cov= X_covmat, size=datarows, check_valid='warn', tol=1e-8)\n",
    "sampled_data = np.hstack((np.ones([datarows,1]), X_matrix, np.expand_dims(X_matrix[:, 0] * X_matrix[:, 1], axis=1) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ,  5.56351044,  7.22094545, 40.17380542],\n",
       "        [ 1.        ,  4.47257664,  1.00233506,  4.48302036],\n",
       "        [ 1.        ,  2.21599463,  1.8523974 ,  4.1049027 ],\n",
       "        ...,\n",
       "        [ 1.        ,  5.69757285,  2.46158286, 14.02504765],\n",
       "        [ 1.        ,  8.22817079,  2.64615415, 21.77300827],\n",
       "        [ 1.        ,  5.0680296 ,  5.49379004, 27.84269052]]),\n",
       " array([ 1.        ,  5.01875685,  4.99680941, 26.1706049 ]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_data, sampled_data.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColDataset(Dataset):\n",
    "    def __init__(self, num_arms, dx, mu_vec, parmCovMat, knownfeatures):\n",
    "        self.knownfeatures      = knownfeatures\n",
    "        self.sampled_data       = np.array(dx, dtype=np.float32) \n",
    "        self.ERROR_VARIANCE_STD = 0.25\n",
    "        self.mu_vec             = mu_vec\n",
    "        self.parmCovMat         = parmCovMat\n",
    "        self.num_actions        = num_arms\n",
    "        if isinstance(self.knownfeatures, np.ndarray ):\n",
    "             self.orgtraffic_known_features     = torch.from_numpy(self.sampled_data[:,self.knownfeatures]).float()\n",
    "        else:\n",
    "             self.orgtraffic_known_features     = torch.from_numpy(self.sampled_data                      ).float()\n",
    "        \n",
    "    def sampled_parm(self, act:int=0, size=1):\n",
    "        # mu_vec, parmCovMat  \n",
    "        return np.random.multivariate_normal(mean= mu_vec[act], cov= parmCovMat[act], size=size, check_valid='warn', tol=1e-8)\n",
    "        #return mu_vec[act] #population coefficients, as if deterministic\n",
    "\n",
    "    def yourORG_as_a_blackBox(self, d, act):\n",
    "        parm    = self.sampled_parm(act=act, size=1)\n",
    "        reward  = np.dot(parm , np.transpose(d)[..., np.newaxis]).ravel()\n",
    "        err     = np.random.normal(loc=0, scale=self.ERROR_VARIANCE_STD, size=1)\n",
    "        reward +=err\n",
    "        return np.array(reward)#[..., np.newaxis]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sampled_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #call recommended action\n",
    "        act  = np.random.choice(np.arange(self.num_actions), size=1)[0] #CHANGE  #self.lp.yourAPI_recommending_actions(mdlidx, context = orgtraffic_known_features, size=len(orgtraffic_known_features))\n",
    "        d_np = self.sampled_data[idx]\n",
    "        d    = self.orgtraffic_known_features[idx]\n",
    "        #pass actions to your organization and measure reward \n",
    "        y    = torch.from_numpy(self.yourORG_as_a_blackBox(d_np, act)).float() \n",
    "        act  = torch.tensor(act, dtype=torch.long)\n",
    "        #print(f\"d - shape - {d.shape}  ; y.shape - \", y.shape)\n",
    "        \n",
    "        return d, act, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mdlnn(torch.nn.Module):\n",
    "    def __init__(self, layersizes, hidden_drops, num_features, num_arms):\n",
    "        super(mdlnn, self).__init__()\n",
    "        \n",
    "        self.num_arms     = num_arms\n",
    "        self.hidden_drops = hidden_drops\n",
    "        self.layersizes   = layersizes\n",
    "        self.num_features = num_features - 1 #exclude intercept term because we use batchnorm and linear layers include bias coeff\n",
    "        \n",
    "        if (self.layersizes[0] ==1)&(len(self.layersizes)==1):\n",
    "            llist = [torch.nn.Linear(self.num_features, 1, bias=True) for i in range(self.num_arms)]\n",
    "            \n",
    "            self.lins = torch.nn.ModuleList(llist)\n",
    "        else:\n",
    "            assert self.layersizes[0] >1, 'When training non-linear model, have atleast 2 nodes in 1st layer'\n",
    "            assert self.layersizes[-1]>1, 'When training non-linear model, do not define final layers as part of this set, define them in output '\n",
    "            llist = [torch.nn.Linear(self.num_features, layersizes[0], bias=True)] #shared layers for all arms\n",
    "            for i in range(len(self.layersizes)-1): #shared layers for all arms\n",
    "                  llist.append(torch.nn.Linear(layersizes[i], layersizes[i+1], bias=True))\n",
    "                    \n",
    "            self.lins = torch.nn.ModuleList(llist)\n",
    "            # batch norm\n",
    "            self.bns_input = torch.nn.BatchNorm1d(self.num_features)\n",
    "\n",
    "            # dropouts\n",
    "            self.drops     = torch.nn.ModuleList([torch.nn.Dropout(drop) for drop in hidden_drops])\n",
    "\n",
    "            #activation\n",
    "            self.actfunc   = torch.nn.LeakyReLU(0.01)\n",
    "            self.output    = torch.nn.ModuleList([torch.nn.Linear(layersizes[-1], 1, bias=True) for i in range(self.num_arms)])\n",
    "            for ll in self.output:\n",
    "                torch.nn.init.xavier_normal_(ll.weight) #torch.nn.init.orthogonal_(ll.weight)\n",
    "        \n",
    "        for ll in self.lins:\n",
    "           torch.nn.init.xavier_normal_(ll.weight) #torch.nn.init.orthogonal_(ll.weight) \n",
    "        \n",
    "    def forward(self, x, act):\n",
    "        if (self.layersizes[0] ==1)&(len(self.layersizes)==1):\n",
    "            x_l = [l(x[act==i,:]) for i, l in enumerate(self.lins)]\n",
    "        else:\n",
    "            #print(\"entered in\")\n",
    "            x = self.bns_input(x)\n",
    "            for i in range(len(self.layersizes)):\n",
    "                x = self.lins[i](x)\n",
    "                #x = self.actfunc(x)\n",
    "                x = self.drops[i](x)\n",
    "            print(x.shape, 'x')\n",
    "            x_l = [l(x[act==i,:]) for i, l in enumerate(self.output)]\n",
    "        return x_l\n",
    "\n",
    "    def forward_custom(self, x):\n",
    "        if 1==0:\n",
    "            x_l = [l(x).detach().numpy() for i, l in enumerate(self.lins)]\n",
    "            return x_l, x.detach().numpy()\n",
    "        else:\n",
    "            #print(\"entered in\")\n",
    "            x = self.bns_input(x)\n",
    "            for i in range(len(self.layersizes)):\n",
    "                x = self.lins[i](x)\n",
    "                #x = self.actfunc(x)\n",
    "                x = self.drops[i](x)\n",
    "            \n",
    "            x_l = [l(x).detach().numpy() for i, l in enumerate(self.output)]\n",
    "            return x_l, x.detach().numpy() #THIS has last layer y_hat (for each arm), AND, L-1 layer representation (which is agnostic of the arm/action). Pass this L-1 as z/context in Linear-TS\n",
    "            \n",
    "        return x_l, np.array([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded and transformed\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "KNOWNFEATURES = 0\n",
    "train_ds = ColDataset(num_arms, sampled_data, mu_vec, parmCovMat, knownfeatures=KNOWNFEATURES) # train_trf_data[:, cat_vars_index], train_trf_data[:, continuous_vars_index], y1)\n",
    "valid_ds = ColDataset(num_arms, sampled_data, mu_vec, parmCovMat, knownfeatures=KNOWNFEATURES)\n",
    "print(\"Data Loaded and transformed\")\n",
    "\n",
    "trn_dataloader = DataLoader(train_ds, batch_size=4000, shuffle=True, num_workers=4)\n",
    "val_dataloader = DataLoader(valid_ds, batch_size=10000000, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if isinstance(KNOWNFEATURES, np.ndarray ):\n",
    "#      mdl = mdlnn(layersizes=[4], hidden_drops=[0.7], num_features=len(KNOWNFEATURES)   , num_arms=num_arms)\n",
    "# else:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mdlnn(\n",
       "  (lins): ModuleList(\n",
       "    (0): Linear(in_features=3, out_features=4, bias=True)\n",
       "  )\n",
       "  (bns_input): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drops): ModuleList(\n",
       "    (0): Dropout(p=0.7, inplace=False)\n",
       "  )\n",
       "  (actfunc): LeakyReLU(negative_slope=0.01)\n",
       "  (output): ModuleList(\n",
       "    (0): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (1): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (3): Linear(in_features=4, out_features=1, bias=True)\n",
       "    (4): Linear(in_features=4, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = mdlnn(layersizes=[4], hidden_drops=[0.7], num_features=sampled_data.shape[1], num_arms=num_arms)\n",
    "mdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(mdl.parameters(), lr=1e-3, momentum=0.9)\n",
    "#optimizer = torch.optim.Adam(list(mdl.parameters()), lr = 1e-3)\n",
    "lrscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)\n",
    "#lrscheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-6, max_lr=1e-3, mode='triangular') #mode='triangular2'\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "mseloss  = F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([4000, 1]) y\n",
      "torch.Size([4000])\n",
      "torch.Size([4000, 4]) x\n",
      "5 p\n",
      "torch.Size([770, 1])\n",
      "5 yl\n",
      "torch.Size([770, 1])\n",
      "torch.Size([2000, 1]) y\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 4]) x\n",
      "5 p\n",
      "torch.Size([367, 1])\n",
      "5 yl\n",
      "torch.Size([367, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-52325a282241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_epoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrn_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 872\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    873\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    912\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tr_loss = []\n",
    "mdl.train()\n",
    "train_offpolicy = []\n",
    "for train_epoch in range(1, 100):\n",
    "    biter = 0\n",
    "    for dx,act,y in trn_dataloader:\n",
    "        print(y.shape, 'y')\n",
    "        print(act.shape)\n",
    "        optimizer.zero_grad()\n",
    "        pred_list      = mdl.forward(dx[:,1:], act)  #exclude intercept/constant\n",
    "        print(len(pred_list), 'p')\n",
    "        print(pred_list[0].shape)\n",
    "        y_list         = [y[act==i,:] for i in range(num_arms)]\n",
    "        print(len(y_list), 'yl')\n",
    "        print(y_list[0].shape)\n",
    "        loss = 0\n",
    "        for i in range(num_arms):\n",
    "            loss += mseloss(pred_list[i].flatten().to(DEVICE), y_list[i].flatten().to(DEVICE))\n",
    "            #print(f'loss - after arm {i} loss is added - {loss}')\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lrscheduler.step()\n",
    "        \n",
    "        biter +=1\n",
    "        #print(f\"train_epoch - {train_epoch} ; biter - {biter} ; loss -- {loss} ; dx - {dx.shape} ; y - {y.shape}\")\n",
    "        \n",
    "        train_offpolicy.append([train_epoch, biter , dx.detach().numpy() , act.flatten().detach().numpy() , y.flatten().detach().numpy() ] )\n",
    "\n",
    "        tr_loss.append([train_epoch, biter, loss.item()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 337.09033203125],\n",
       " [1, 2, 331.8153076171875],\n",
       " [1, 3, 292.12847900390625],\n",
       " [2, 1, 296.585205078125],\n",
       " [2, 2, 280.14599609375],\n",
       " [2, 3, 271.2704772949219],\n",
       " [3, 1, 250.09718322753906],\n",
       " [3, 2, 244.40707397460938],\n",
       " [3, 3, 227.0341796875],\n",
       " [4, 1, 220.47055053710938],\n",
       " [4, 2, 216.134765625],\n",
       " [4, 3, 200.00865173339844],\n",
       " [5, 1, 187.7088165283203],\n",
       " [5, 2, 172.44076538085938],\n",
       " [5, 3, 168.01290893554688],\n",
       " [6, 1, 163.15493774414062],\n",
       " [6, 2, 160.61769104003906],\n",
       " [6, 3, 171.02984619140625],\n",
       " [7, 1, 155.96754455566406],\n",
       " [7, 2, 157.88986206054688],\n",
       " [7, 3, 153.60079956054688],\n",
       " [8, 1, 142.91201782226562],\n",
       " [8, 2, 145.76634216308594],\n",
       " [8, 3, 152.1350860595703],\n",
       " [9, 1, 147.83865356445312],\n",
       " [9, 2, 143.379638671875],\n",
       " [9, 3, 146.55874633789062],\n",
       " [10, 1, 141.83526611328125],\n",
       " [10, 2, 136.55242919921875],\n",
       " [10, 3, 134.05552673339844],\n",
       " [11, 1, 132.486572265625],\n",
       " [11, 2, 137.46444702148438],\n",
       " [11, 3, 137.78561401367188],\n",
       " [12, 1, 134.34690856933594],\n",
       " [12, 2, 131.43739318847656],\n",
       " [12, 3, 133.68919372558594],\n",
       " [13, 1, 132.84832763671875],\n",
       " [13, 2, 130.88333129882812],\n",
       " [13, 3, 140.85595703125],\n",
       " [14, 1, 127.2646255493164],\n",
       " [14, 2, 124.42464447021484],\n",
       " [14, 3, 130.55894470214844],\n",
       " [15, 1, 126.24715423583984],\n",
       " [15, 2, 127.11952209472656],\n",
       " [15, 3, 136.4320068359375],\n",
       " [16, 1, 122.21955871582031],\n",
       " [16, 2, 122.58709716796875],\n",
       " [16, 3, 127.15422058105469],\n",
       " [17, 1, 120.11907196044922],\n",
       " [17, 2, 123.33014678955078],\n",
       " [17, 3, 121.5049819946289],\n",
       " [18, 1, 119.63197326660156],\n",
       " [18, 2, 124.86965942382812],\n",
       " [18, 3, 124.44432067871094],\n",
       " [19, 1, 129.48777770996094],\n",
       " [19, 2, 115.87660217285156],\n",
       " [19, 3, 124.22942352294922],\n",
       " [20, 1, 122.70467376708984],\n",
       " [20, 2, 118.01831817626953],\n",
       " [20, 3, 123.03450775146484],\n",
       " [21, 1, 118.5784912109375],\n",
       " [21, 2, 117.5035400390625],\n",
       " [21, 3, 118.88291931152344],\n",
       " [22, 1, 121.2779312133789],\n",
       " [22, 2, 118.31273651123047],\n",
       " [22, 3, 119.41368865966797],\n",
       " [23, 1, 116.63700866699219],\n",
       " [23, 2, 116.24864196777344],\n",
       " [23, 3, 111.84269714355469],\n",
       " [24, 1, 116.84857177734375],\n",
       " [24, 2, 116.26152038574219],\n",
       " [24, 3, 126.43466186523438],\n",
       " [25, 1, 116.1866683959961],\n",
       " [25, 2, 112.86105346679688],\n",
       " [25, 3, 116.98123168945312],\n",
       " [26, 1, 115.59988403320312],\n",
       " [26, 2, 112.06047058105469],\n",
       " [26, 3, 110.8031234741211],\n",
       " [27, 1, 115.45035552978516],\n",
       " [27, 2, 106.77093505859375],\n",
       " [27, 3, 115.67115783691406],\n",
       " [28, 1, 114.63581848144531],\n",
       " [28, 2, 114.2365493774414],\n",
       " [28, 3, 114.4524917602539],\n",
       " [29, 1, 111.19810485839844],\n",
       " [29, 2, 110.08361053466797],\n",
       " [29, 3, 106.7357177734375],\n",
       " [30, 1, 112.97383117675781],\n",
       " [30, 2, 116.162353515625],\n",
       " [30, 3, 105.33586120605469],\n",
       " [31, 1, 106.81472778320312],\n",
       " [31, 2, 111.53485107421875],\n",
       " [31, 3, 107.39778137207031],\n",
       " [32, 1, 105.84234619140625],\n",
       " [32, 2, 108.11695861816406],\n",
       " [32, 3, 106.0024642944336],\n",
       " [33, 1, 108.9980697631836],\n",
       " [33, 2, 112.00468444824219],\n",
       " [33, 3, 115.49891662597656],\n",
       " [34, 1, 113.3114013671875],\n",
       " [34, 2, 113.29670715332031],\n",
       " [34, 3, 107.40467071533203],\n",
       " [35, 1, 107.77433776855469],\n",
       " [35, 2, 103.24102783203125],\n",
       " [35, 3, 106.73048400878906],\n",
       " [36, 1, 108.71432495117188],\n",
       " [36, 2, 108.4426040649414],\n",
       " [36, 3, 105.46217346191406],\n",
       " [37, 1, 111.11053466796875],\n",
       " [37, 2, 108.44165802001953],\n",
       " [37, 3, 110.34500885009766],\n",
       " [38, 1, 108.91614532470703],\n",
       " [38, 2, 106.07716369628906],\n",
       " [38, 3, 111.10295867919922],\n",
       " [39, 1, 108.50297546386719],\n",
       " [39, 2, 107.40658569335938],\n",
       " [39, 3, 107.60014343261719],\n",
       " [40, 1, 110.4498519897461],\n",
       " [40, 2, 106.82823181152344],\n",
       " [40, 3, 98.85908508300781],\n",
       " [41, 1, 106.05508422851562],\n",
       " [41, 2, 103.59969329833984],\n",
       " [41, 3, 108.41136932373047],\n",
       " [42, 1, 105.28315734863281],\n",
       " [42, 2, 106.87738037109375],\n",
       " [42, 3, 111.38533020019531],\n",
       " [43, 1, 103.645263671875],\n",
       " [43, 2, 106.9458236694336],\n",
       " [43, 3, 102.14762115478516],\n",
       " [44, 1, 107.41519927978516],\n",
       " [44, 2, 103.52425384521484],\n",
       " [44, 3, 106.11141204833984],\n",
       " [45, 1, 104.42215728759766],\n",
       " [45, 2, 106.75487518310547],\n",
       " [45, 3, 98.42327880859375],\n",
       " [46, 1, 105.9179458618164],\n",
       " [46, 2, 109.56954193115234],\n",
       " [46, 3, 107.34333038330078],\n",
       " [47, 1, 108.80597686767578],\n",
       " [47, 2, 105.2156982421875],\n",
       " [47, 3, 106.49805450439453],\n",
       " [48, 1, 105.64347839355469],\n",
       " [48, 2, 100.72998046875],\n",
       " [48, 3, 101.24446105957031],\n",
       " [49, 1, 103.95474243164062],\n",
       " [49, 2, 100.1878662109375],\n",
       " [49, 3, 100.96736145019531],\n",
       " [50, 1, 101.14965057373047],\n",
       " [50, 2, 101.01512145996094],\n",
       " [50, 3, 104.98635864257812],\n",
       " [51, 1, 99.36075592041016],\n",
       " [51, 2, 103.5249252319336],\n",
       " [51, 3, 101.35067749023438],\n",
       " [52, 1, 101.53621673583984],\n",
       " [52, 2, 100.74028778076172],\n",
       " [52, 3, 102.07719421386719],\n",
       " [53, 1, 104.91697692871094],\n",
       " [53, 2, 102.06221008300781],\n",
       " [53, 3, 101.88215637207031],\n",
       " [54, 1, 98.84536743164062],\n",
       " [54, 2, 98.96444702148438],\n",
       " [54, 3, 100.84278106689453],\n",
       " [55, 1, 102.68401336669922],\n",
       " [55, 2, 102.36646270751953],\n",
       " [55, 3, 102.62360382080078],\n",
       " [56, 1, 103.21083068847656],\n",
       " [56, 2, 101.96771240234375],\n",
       " [56, 3, 104.37689208984375],\n",
       " [57, 1, 101.05117797851562],\n",
       " [57, 2, 102.6248779296875],\n",
       " [57, 3, 98.12065887451172],\n",
       " [58, 1, 96.73056030273438],\n",
       " [58, 2, 101.06159973144531],\n",
       " [58, 3, 103.08610534667969],\n",
       " [59, 1, 102.2195816040039],\n",
       " [59, 2, 100.02070617675781],\n",
       " [59, 3, 103.37471008300781],\n",
       " [60, 1, 104.03205108642578],\n",
       " [60, 2, 100.043701171875],\n",
       " [60, 3, 102.84170532226562],\n",
       " [61, 1, 101.74895477294922],\n",
       " [61, 2, 103.58067321777344],\n",
       " [61, 3, 101.77659606933594],\n",
       " [62, 1, 100.23623657226562],\n",
       " [62, 2, 102.70040893554688],\n",
       " [62, 3, 107.62745666503906],\n",
       " [63, 1, 105.58422088623047],\n",
       " [63, 2, 100.20480346679688],\n",
       " [63, 3, 102.34503936767578],\n",
       " [64, 1, 97.1754150390625],\n",
       " [64, 2, 101.70529174804688],\n",
       " [64, 3, 104.3650894165039],\n",
       " [65, 1, 98.04869842529297],\n",
       " [65, 2, 104.01272583007812],\n",
       " [65, 3, 100.78413391113281],\n",
       " [66, 1, 100.29928588867188],\n",
       " [66, 2, 104.78146362304688],\n",
       " [66, 3, 102.06291961669922],\n",
       " [67, 1, 98.75067138671875],\n",
       " [67, 2, 100.1921615600586],\n",
       " [67, 3, 100.918212890625],\n",
       " [68, 1, 102.66746520996094],\n",
       " [68, 2, 96.14227294921875],\n",
       " [68, 3, 93.31761932373047],\n",
       " [69, 1, 101.22444152832031],\n",
       " [69, 2, 101.55057525634766],\n",
       " [69, 3, 100.46504974365234],\n",
       " [70, 1, 105.36160278320312],\n",
       " [70, 2, 102.54581451416016],\n",
       " [70, 3, 103.88630676269531],\n",
       " [71, 1, 98.23359680175781],\n",
       " [71, 2, 96.71537780761719],\n",
       " [71, 3, 96.44686889648438],\n",
       " [72, 1, 96.28572845458984],\n",
       " [72, 2, 101.1390609741211],\n",
       " [72, 3, 103.69805145263672],\n",
       " [73, 1, 97.60609436035156],\n",
       " [73, 2, 102.74655151367188],\n",
       " [73, 3, 99.39022827148438],\n",
       " [74, 1, 101.49198913574219],\n",
       " [74, 2, 98.48310852050781],\n",
       " [74, 3, 110.19327545166016],\n",
       " [75, 1, 98.48365783691406],\n",
       " [75, 2, 100.38168334960938],\n",
       " [75, 3, 99.7810287475586],\n",
       " [76, 1, 101.28300476074219],\n",
       " [76, 2, 100.47575378417969],\n",
       " [76, 3, 104.50984954833984],\n",
       " [77, 1, 100.90477752685547],\n",
       " [77, 2, 103.84596252441406],\n",
       " [77, 3, 104.11190795898438],\n",
       " [78, 1, 103.2489013671875],\n",
       " [78, 2, 94.59016418457031],\n",
       " [78, 3, 99.57014465332031],\n",
       " [79, 1, 100.09838104248047],\n",
       " [79, 2, 101.15316772460938],\n",
       " [79, 3, 101.01087951660156],\n",
       " [80, 1, 100.10206604003906],\n",
       " [80, 2, 99.45391082763672],\n",
       " [80, 3, 107.47012329101562],\n",
       " [81, 1, 98.02877807617188],\n",
       " [81, 2, 99.6998062133789],\n",
       " [81, 3, 99.9285888671875],\n",
       " [82, 1, 97.96165466308594],\n",
       " [82, 2, 96.92929077148438],\n",
       " [82, 3, 92.3816909790039],\n",
       " [83, 1, 100.51588439941406],\n",
       " [83, 2, 100.57159423828125],\n",
       " [83, 3, 99.26060485839844],\n",
       " [84, 1, 97.83580780029297],\n",
       " [84, 2, 95.41271209716797],\n",
       " [84, 3, 101.23011016845703],\n",
       " [85, 1, 97.80999755859375],\n",
       " [85, 2, 96.84158325195312],\n",
       " [85, 3, 93.75338745117188],\n",
       " [86, 1, 96.56915283203125],\n",
       " [86, 2, 96.63117980957031],\n",
       " [86, 3, 100.05506896972656],\n",
       " [87, 1, 98.67979431152344],\n",
       " [87, 2, 94.65652465820312],\n",
       " [87, 3, 99.03340148925781],\n",
       " [88, 1, 97.71380615234375],\n",
       " [88, 2, 101.52693939208984],\n",
       " [88, 3, 102.86528015136719],\n",
       " [89, 1, 95.73480224609375],\n",
       " [89, 2, 99.00907897949219],\n",
       " [89, 3, 101.25033569335938],\n",
       " [90, 1, 98.41613006591797],\n",
       " [90, 2, 100.79624938964844],\n",
       " [90, 3, 98.95065307617188],\n",
       " [91, 1, 96.50566101074219],\n",
       " [91, 2, 100.0063247680664],\n",
       " [91, 3, 99.5664291381836],\n",
       " [92, 1, 99.3658676147461],\n",
       " [92, 2, 101.3386001586914],\n",
       " [92, 3, 97.92912292480469],\n",
       " [93, 1, 98.30680084228516],\n",
       " [93, 2, 97.66700744628906],\n",
       " [93, 3, 95.48147583007812],\n",
       " [94, 1, 99.99012756347656],\n",
       " [94, 2, 96.47528076171875],\n",
       " [94, 3, 98.31843566894531],\n",
       " [95, 1, 93.54911804199219],\n",
       " [95, 2, 91.72953796386719],\n",
       " [95, 3, 99.36032104492188],\n",
       " [96, 1, 92.71819305419922],\n",
       " [96, 2, 99.76847839355469],\n",
       " [96, 3, 95.7609634399414],\n",
       " [97, 1, 95.48233795166016],\n",
       " [97, 2, 99.42655944824219],\n",
       " [97, 3, 98.44021606445312],\n",
       " [98, 1, 97.50292205810547],\n",
       " [98, 2, 94.36196899414062],\n",
       " [98, 3, 98.3553695678711],\n",
       " [99, 1, 94.41838836669922],\n",
       " [99, 2, 99.66539001464844],\n",
       " [99, 3, 96.20748138427734]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='darkblue' size=7 > Sampler </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearPosteriorSampling():\n",
    "    \"\"\"\n",
    "    NN + linear\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, num_actions, sigmultiplier= 'fixed', diagmat =False, scaled_down_error_variance= 0.1, R=0.5):\n",
    "\n",
    "        #self.hparams       = hparams\n",
    "        self.num_features  = num_features #number of nodes/output in L-1 layer of NN\n",
    "        self.num_actions   = num_actions\n",
    "        \n",
    "        self.diagmat       = diagmat\n",
    "        self.sigmultiplier = sigmultiplier\n",
    "\n",
    "        self.scaled_down_error_variance = scaled_down_error_variance\n",
    "\n",
    "        # No prior/ridge lambda, have set this to zero here.\n",
    "        self._lambda_prior = 0.0001 #It can be set to higher value to bolster like a ridge regression \n",
    "\n",
    "        self.mu  = [ np.zeros(self.num_features)\n",
    "                                        for _ in range(self.num_actions) ]\n",
    "\n",
    "        self.cov = [(1.0 / self._lambda_prior) * np.eye(self.num_features)\n",
    "                                        for _ in range(self.num_actions)]\n",
    "\n",
    "        self.precision = [self._lambda_prior * np.eye(self.num_features)\n",
    "                                        for _ in range(self.num_actions) ]\n",
    "        \n",
    "        self.R       = R\n",
    "        self.epsilon = 0.1\n",
    "        self.delta   = 0.5\n",
    "        self.t       = 1\n",
    "        \n",
    "        self.sig_sq = self.R * np.sqrt((24 / self.epsilon) * self.num_features * np.log(self.t / self.delta))\n",
    "        \n",
    "        if self.sigmultiplier =='fixed':\n",
    "             self.sig_sq_2 = [self.sig_sq  for _ in range(self.num_actions) ]\n",
    "        elif (self.sigmultiplier =='unbiased_error_variance') or (self.sigmultiplier =='scaled_down_error_variance'):\n",
    "             self.sig_sq_2 = [0.01  for _ in range(self.num_actions) ]\n",
    "\n",
    "        self.t = 0\n",
    "        self.data_cumulative = {}\n",
    "        for i in range(self.num_actions):\n",
    "            self.data_cumulative[i] = {'context': np.array([]) , \n",
    "                                        'reward': np.array([])}\n",
    "            \n",
    "        self.sig_sq_2_BKP_list = []\n",
    "\n",
    "    def update_offline_data(self, context, action, reward):\n",
    "        for i in range(self.num_actions):\n",
    "            if len(self.data_cumulative[i]['context'])==0:\n",
    "                self.data_cumulative[i]['context'] = copy.deepcopy(context[(action==i).ravel(),:]) \n",
    "                self.data_cumulative[i]['reward']  = copy.deepcopy(reward[(action==i).ravel(),:]) \n",
    "            else:\n",
    "                #assert self.data_cumulative[i]['context'].shape==context.shape, 'Shape of numpy arrays of context/features does not match'\n",
    "                self.data_cumulative[i]['context'] =  np.vstack((self.data_cumulative[i]['context'], context[(action==i).ravel(),:])) \n",
    "                self.data_cumulative[i]['reward']  =  np.vstack((self.data_cumulative[i]['reward'] , reward[(action==i).ravel(),:])) \n",
    "                \n",
    "    def yourAPI_recommending_actions(self, mdlidx , context, size=1):\n",
    "        if mdlidx ==1: #random\n",
    "            if size==1:\n",
    "                return np.random.choice(np.arange(self.num_actions), size=size)[0]\n",
    "            else:\n",
    "                return np.random.choice(np.arange(self.num_actions), size=size)\n",
    "        elif mdlidx ==2: #Linear TS\n",
    "            return self.recommend_action(context)\n",
    "\n",
    "    def recommend_action(self, context):\n",
    "        \"\"\"Samples beta's from posterior, and chooses best action accordingly.\"\"\"\n",
    "\n",
    "        ## pass through NN\n",
    "        mdl.eval()\n",
    "        _, z = mdl.forward_custom(torch.from_numpy(context).float()[:,1:])\n",
    "        try:\n",
    "            if self.diagmat ==False:\n",
    "                beta_s = [ np.random.multivariate_normal(self.mu[i], self.sig_sq_2[i] * self.cov[i]) \n",
    "                                         for i in range(self.num_actions) ]\n",
    "            else:\n",
    "                beta_s = [ np.random.multivariate_normal(self.mu[i], self.sig_sq_2[i] * (np.diag(self.cov[i]) * np.eye(self.num_features)) ) \n",
    "                                         for i in range(self.num_actions) ]\n",
    "                \n",
    "        except np.linalg.LinAlgError as e:\n",
    "            # Sampling could fail if covariance matrix is not positive definite\n",
    "            print('Exception when sampling for {}.'.format(self.name))\n",
    "            print('Details: {} | {}.'.format(e.message, e.args))\n",
    "            beta_s = [ np.random.multivariate_normal(np.zeros((self.num_features)), np.eye(self.num_features))\n",
    "                                           for i in range(self.num_actions) ]\n",
    "\n",
    "        # Apply Thompson Sampling principal to last-layer representation\n",
    "        vals = [ np.dot(beta_s[i], z.T) for i in range(self.num_actions) ]\n",
    "        return np.argmax(vals,axis=0)\n",
    "\n",
    "    def update(self, context, action, reward):\n",
    "        \"\"\"Updates the posterior using linear bayesian regression formula.\"\"\"\n",
    "\n",
    "        #stack up new data\n",
    "        self.t += 1\n",
    "        self.update_offline_data(context, action, reward)\n",
    "\n",
    "        # Update the Linear Regression (NOT a bayesian update, take full batch and update)\n",
    "        for action_v in range(self.num_actions):\n",
    "            # pull up full batch data from the offline training stack/batch\n",
    "            z, y = self.data_cumulative[action_v]['context'], self.data_cumulative[action_v]['reward']  \n",
    "\n",
    "            #pass through NN\n",
    "            mdl.eval()\n",
    "            _, z = mdl.forward_custom(torch.from_numpy(z).float()[:,1:])\n",
    "            \n",
    "            #OLS Regression updates\n",
    "            s = np.dot(z.T, z)\n",
    "            \n",
    "            #print(f\"iter - {self.t} , action_v - {action_v} , s matrix - {s}\")\n",
    "            precision_a = s + self._lambda_prior * np.eye(self.num_features)\n",
    "            #precision_a = s \n",
    "            cov_a       = np.linalg.inv(precision_a)\n",
    "            mu_a        = np.dot(cov_a, np.dot(z.T, y)).ravel()\n",
    "\n",
    "            # Store parms\n",
    "            self.mu[action_v]        = mu_a\n",
    "            self.cov[action_v]       = cov_a\n",
    "            self.precision[action_v] = precision_a\n",
    "\n",
    "            #unbiased error variance\n",
    "            y_hat                   = np.dot(z, mu_a).ravel()\n",
    "            err                     = y.ravel() - y_hat\n",
    "            sig_sq_2                = np.dot(err.T, err) /(z.shape[0] - len(mu_a.ravel())) #unbiased_error_variance\n",
    "\n",
    "            if self.sigmultiplier =='fixed':\n",
    "                pass #\n",
    "            elif self.sigmultiplier =='unbiased_error_variance':\n",
    "                self.sig_sq_2[action_v] = sig_sq_2\n",
    "            elif self.sigmultiplier =='scaled_down_error_variance':\n",
    "                self.sig_sq_2[action_v] = self.scaled_down_error_variance * sig_sq_2\n",
    "\n",
    "            self.sig_sq_2_BKP_list.append([self.t, action_v, z.shape[0], self.sig_sq_2[action_v] , self.sig_sq_2[i] * np.diag(self.cov[i]) ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class training_module():\n",
    "    def __init__(self, sampled_data, num_features=4, num_actions=5, sigmultiplier= 'fixed', diagmat =False, scaled_down_error_variance= 0.1, R=0.5, knownfeatures=0):\n",
    "        #\n",
    "        self.lp             = LinearPosteriorSampling(num_features=4, num_actions=5, sigmultiplier= 'fixed', diagmat =False, scaled_down_error_variance= 0.1, R=0.5)\n",
    "        self.sampled_data   = sampled_data\n",
    "        self.num_features   = num_features\n",
    "        self.ERROR_VARIANCE_STD = 0.25\n",
    "        if isinstance(knownfeatures, np.ndarray ) or np.isscalar(knownfeatures):\n",
    "            self.knownfeatures  = knownfeatures\n",
    "        else:\n",
    "            assert 0==1, 'Either pass a scalar in knownfeatures OR pass numpy array indices of columns that should be used in the feature set'\n",
    "\n",
    "    def sampled_parm(self, act:int=0, size=1):\n",
    "        # mu_vec, parmCovMat  \n",
    "        return np.random.multivariate_normal(mean= mu_vec[act], cov= parmCovMat[act], size=size, check_valid='warn', tol=1e-8)\n",
    "\n",
    "    def yourORG_as_a_blackBox(self, d, rec_actions):\n",
    "        rewards_given_action = []\n",
    "        for i, r in enumerate(d):\n",
    "            parm    = self.sampled_parm(act=rec_actions[i], size=1)\n",
    "            reward  = np.dot(parm , np.transpose(r)[..., np.newaxis]).ravel()\n",
    "            err     = np.random.normal(loc=0, scale=self.ERROR_VARIANCE_STD, size=1)\n",
    "            reward +=err\n",
    "            rewards_given_action.append([np.asscalar(reward)])\n",
    "        return np.array(rewards_given_action)\n",
    "    \n",
    "    def run_each_batch(self, btchsize, mdlidx=1):\n",
    "        orgtrf_idx                    = np.random.choice(np.arange(len(self.sampled_data)), size= btchsize, replace=False,) #d[btchidx]\n",
    "        if isinstance(self.knownfeatures, np.ndarray ):\n",
    "             orgtraffic_known_features     = self.sampled_data[orgtrf_idx, :][:,self.knownfeatures]\n",
    "        else:\n",
    "             orgtraffic_known_features     = self.sampled_data[orgtrf_idx]\n",
    "        orgtraffic_w_unknown_features = self.sampled_data[orgtrf_idx]\n",
    "        rec_actions                   = self.lp.yourAPI_recommending_actions(mdlidx, context = orgtraffic_known_features, size=len(orgtraffic_known_features))\n",
    "\n",
    "        rewards_seen_in_data          = self.yourORG_as_a_blackBox(orgtraffic_w_unknown_features, rec_actions)\n",
    "        return orgtraffic_known_features, rec_actions[..., np.newaxis], rewards_seen_in_data\n",
    "\n",
    "    def training_routine(self, btchsize, num_batches):\n",
    "        batchnum  = 0\n",
    "        #BATCHSIZE = btchsize\n",
    "        d, a, r   = self.run_each_batch(btchsize= btchsize, mdlidx =1) # random selection of actions\n",
    "            \n",
    "        #update offline stacked data in lp object with new data, re-estimate regression coefficients (batch 0)\n",
    "        self.lp.update(d, a, r)\n",
    "\n",
    "        #batch 1 to N - explore exploit, update parameters\n",
    "        reward_sum_list = [] \n",
    "        reward_sum_list.append([batchnum, r.ravel().mean(), r.ravel().sum()])\n",
    "        for batchnum in range(1, num_batches): \n",
    "               #explore exploit\n",
    "               d, a, r   = self.run_each_batch(btchsize= btchsize, mdlidx =2) \n",
    "               reward_sum_list.append([batchnum, r.ravel().mean(), r.ravel().sum()])\n",
    "               #update offline stacked data in lp object with new data, re-estimate regression coefficients\n",
    "               self.lp.update(d, a, r)\n",
    "\n",
    "        dfres = pd.DataFrame(reward_sum_list, columns = ['batchnum','mean_reward','sum_reward'])\n",
    "\n",
    "        variance_parms = pd.DataFrame(self.lp.sig_sq_2_BKP_list, columns=['iter','act','N', 'sig_square', 'sig_cov_prod' ])\n",
    "\n",
    "        self.dfres          = dfres\n",
    "        self.variance_parms = variance_parms\n",
    "\n",
    "        #return dfres, variance_parms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel_launcher.py:24: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21min 52s, sys: 19.1 s, total: 22min 11s\n",
      "Wall time: 5min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "allresults         = []\n",
    "R                  = 0.5\n",
    "scale_down_err_var = 1\n",
    "NUM_TRIALS         = 5\n",
    "knowf              = 0\n",
    "for sm in ['scaled_down_error_variance']:\n",
    "    for diagmat in [True, False]:\n",
    "            for scale_down_err_var in [0.1, 0.25, 0.5, 0.8]:\n",
    "                for trialnum in np.arange(1,NUM_TRIALS):\n",
    "                    trn_m = training_module(sampled_data, \n",
    "                                           num_features  = mdl.layersizes[-1], #sampled_data.shape[1], \n",
    "                                           num_actions   = num_arms, \n",
    "                                           sigmultiplier = sm, \n",
    "                                           diagmat       = diagmat, \n",
    "                                           scaled_down_error_variance = scale_down_err_var, #relevant only for scaled_error_variance \n",
    "                                           R                          = 0.5, \n",
    "                                           knownfeatures              = 0)\n",
    "                    trn_m.training_routine(btchsize=300, num_batches=100)\n",
    "                    arm_dist                  = pd.DataFrame(np.expand_dims(trn_m.variance_parms.pivot(index='iter',columns='act', values=['N']).iloc[-1,:].values, axis=0), \\\n",
    "                                                                         columns=['arm_n_'+str(int(i)) for i in np.arange(num_arms)])\n",
    "                    arm_dist['rew_mean']      = trn_m.dfres.iloc[-10:,:].mean_reward.mean()\n",
    "                    arm_dist['rew_std' ]      = trn_m.dfres.iloc[-10:,:].mean_reward.std()\n",
    "                    arm_dist['sigmultiplier'] = (sm if sm !='scaled_down_error_variance' else 'scldwn_errvar_'+ str(scale_down_err_var) ) + (\"_diagmat\" if diagmat==True else \"_fullMat\")\n",
    "                    arm_dist['trialnum']      = trialnum\n",
    "                    arm_dist['R']             = str(R)\n",
    "                    arm_dist['scale_down_err_var'] = str(scale_down_err_var)\n",
    "                    arm_dist['knownfeatures']      = str(knowf)\n",
    "                    allresults.append(arm_dist)\n",
    "for sm in ['fixed']:\n",
    "    for diagmat in [True, False]:\n",
    "            for R in [0.1, 0.2, 0.5]:\n",
    "                for trialnum in np.arange(1,NUM_TRIALS):\n",
    "                    trn_m = training_module(sampled_data, \n",
    "                                           num_features  = mdl.layersizes[-1], #sampled_data.shape[1], \n",
    "                                           num_actions   = num_arms, \n",
    "                                           sigmultiplier = sm, \n",
    "                                           diagmat       = diagmat, \n",
    "                                           scaled_down_error_variance = scale_down_err_var, #relevant only for scaled_error_variance \n",
    "                                           R                          = R, \n",
    "                                           knownfeatures              = 0)\n",
    "                    trn_m.training_routine(btchsize=300, num_batches=100)\n",
    "                    arm_dist                  = pd.DataFrame(np.expand_dims(trn_m.variance_parms.pivot(index='iter',columns='act', values=['N']).iloc[-1,:].values, axis=0), \\\n",
    "                                                                         columns=['arm_n_'+str(int(i)) for i in np.arange(num_arms)])\n",
    "                    arm_dist['rew_mean']      = trn_m.dfres.iloc[-10:,:].mean_reward.mean()\n",
    "                    arm_dist['rew_std' ]      = trn_m.dfres.iloc[-10:,:].mean_reward.std()\n",
    "                    arm_dist['sigmultiplier'] = (sm if sm !='scaled_down_error_variance' else 'scldwn_errvar_'+ str(scale_down_err_var) ) + (\"_diagmat\" if diagmat==True else \"_fullMat\")\n",
    "                    arm_dist['trialnum']      = trialnum\n",
    "                    arm_dist['R']             = str(R)\n",
    "                    arm_dist['scale_down_err_var'] = str(scale_down_err_var)\n",
    "                    arm_dist['knownfeatures']      = str(knowf)\n",
    "                    allresults.append(arm_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "allresults_bkp = copy.deepcopy(allresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 12)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults = pd.concat(allresults)\n",
    "allresults.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arm_n_0</th>\n",
       "      <th>arm_n_1</th>\n",
       "      <th>arm_n_2</th>\n",
       "      <th>arm_n_3</th>\n",
       "      <th>arm_n_4</th>\n",
       "      <th>rew_mean</th>\n",
       "      <th>rew_std</th>\n",
       "      <th>sigmultiplier</th>\n",
       "      <th>trialnum</th>\n",
       "      <th>R</th>\n",
       "      <th>scale_down_err_var</th>\n",
       "      <th>knownfeatures</th>\n",
       "      <th>unq_series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>418</td>\n",
       "      <td>142</td>\n",
       "      <td>1725</td>\n",
       "      <td>236</td>\n",
       "      <td>27479</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.30</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>375</td>\n",
       "      <td>288</td>\n",
       "      <td>2132</td>\n",
       "      <td>348</td>\n",
       "      <td>26857</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.33</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>254</td>\n",
       "      <td>318</td>\n",
       "      <td>1789</td>\n",
       "      <td>549</td>\n",
       "      <td>27090</td>\n",
       "      <td>8.00</td>\n",
       "      <td>0.29</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>980</td>\n",
       "      <td>435</td>\n",
       "      <td>1761</td>\n",
       "      <td>941</td>\n",
       "      <td>25883</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.32</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>389</td>\n",
       "      <td>333</td>\n",
       "      <td>1126</td>\n",
       "      <td>920</td>\n",
       "      <td>27232</td>\n",
       "      <td>7.87</td>\n",
       "      <td>0.26</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>225</td>\n",
       "      <td>195</td>\n",
       "      <td>1165</td>\n",
       "      <td>886</td>\n",
       "      <td>27529</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.31</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>359</td>\n",
       "      <td>340</td>\n",
       "      <td>1316</td>\n",
       "      <td>1143</td>\n",
       "      <td>26842</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.22</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>189</td>\n",
       "      <td>1202</td>\n",
       "      <td>1399</td>\n",
       "      <td>26725</td>\n",
       "      <td>7.82</td>\n",
       "      <td>0.24</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1140</td>\n",
       "      <td>211</td>\n",
       "      <td>1813</td>\n",
       "      <td>471</td>\n",
       "      <td>26365</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.28</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>434</td>\n",
       "      <td>114</td>\n",
       "      <td>1175</td>\n",
       "      <td>1463</td>\n",
       "      <td>26814</td>\n",
       "      <td>7.68</td>\n",
       "      <td>0.25</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>878</td>\n",
       "      <td>112</td>\n",
       "      <td>1446</td>\n",
       "      <td>1164</td>\n",
       "      <td>26400</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.32</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>607</td>\n",
       "      <td>345</td>\n",
       "      <td>1341</td>\n",
       "      <td>458</td>\n",
       "      <td>27249</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.26</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1367</td>\n",
       "      <td>593</td>\n",
       "      <td>1016</td>\n",
       "      <td>559</td>\n",
       "      <td>26465</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0.33</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>336</td>\n",
       "      <td>141</td>\n",
       "      <td>1144</td>\n",
       "      <td>955</td>\n",
       "      <td>27424</td>\n",
       "      <td>7.85</td>\n",
       "      <td>0.27</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312</td>\n",
       "      <td>198</td>\n",
       "      <td>847</td>\n",
       "      <td>309</td>\n",
       "      <td>28334</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.39</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410</td>\n",
       "      <td>163</td>\n",
       "      <td>1784</td>\n",
       "      <td>858</td>\n",
       "      <td>26785</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.25</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>224</td>\n",
       "      <td>1755</td>\n",
       "      <td>1444</td>\n",
       "      <td>26277</td>\n",
       "      <td>7.72</td>\n",
       "      <td>0.28</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>514</td>\n",
       "      <td>383</td>\n",
       "      <td>1656</td>\n",
       "      <td>310</td>\n",
       "      <td>27137</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.21</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567</td>\n",
       "      <td>565</td>\n",
       "      <td>1578</td>\n",
       "      <td>548</td>\n",
       "      <td>26742</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0.25</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>261</td>\n",
       "      <td>186</td>\n",
       "      <td>937</td>\n",
       "      <td>1150</td>\n",
       "      <td>27466</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.26</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.1_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>984</td>\n",
       "      <td>462</td>\n",
       "      <td>1485</td>\n",
       "      <td>611</td>\n",
       "      <td>26458</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.34</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1241</td>\n",
       "      <td>137</td>\n",
       "      <td>1493</td>\n",
       "      <td>1156</td>\n",
       "      <td>25973</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.25</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446</td>\n",
       "      <td>257</td>\n",
       "      <td>1462</td>\n",
       "      <td>1115</td>\n",
       "      <td>26720</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.30</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446</td>\n",
       "      <td>119</td>\n",
       "      <td>1809</td>\n",
       "      <td>1182</td>\n",
       "      <td>26444</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.27</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.25_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>589</td>\n",
       "      <td>386</td>\n",
       "      <td>1976</td>\n",
       "      <td>697</td>\n",
       "      <td>26352</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.23</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>723</td>\n",
       "      <td>129</td>\n",
       "      <td>944</td>\n",
       "      <td>1082</td>\n",
       "      <td>27122</td>\n",
       "      <td>7.74</td>\n",
       "      <td>0.27</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>308</td>\n",
       "      <td>296</td>\n",
       "      <td>1600</td>\n",
       "      <td>1014</td>\n",
       "      <td>26782</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.18</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>872</td>\n",
       "      <td>257</td>\n",
       "      <td>1347</td>\n",
       "      <td>592</td>\n",
       "      <td>26932</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.31</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.5_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>734</td>\n",
       "      <td>217</td>\n",
       "      <td>1642</td>\n",
       "      <td>511</td>\n",
       "      <td>26896</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.36</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1104</td>\n",
       "      <td>131</td>\n",
       "      <td>1596</td>\n",
       "      <td>928</td>\n",
       "      <td>26241</td>\n",
       "      <td>7.91</td>\n",
       "      <td>0.30</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>927</td>\n",
       "      <td>129</td>\n",
       "      <td>1108</td>\n",
       "      <td>694</td>\n",
       "      <td>27142</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.23</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>654</td>\n",
       "      <td>481</td>\n",
       "      <td>821</td>\n",
       "      <td>809</td>\n",
       "      <td>27235</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.24</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>scldwn_errvar_0.8_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>314</td>\n",
       "      <td>291</td>\n",
       "      <td>1512</td>\n",
       "      <td>944</td>\n",
       "      <td>26939</td>\n",
       "      <td>7.82</td>\n",
       "      <td>0.25</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>329</td>\n",
       "      <td>173</td>\n",
       "      <td>1162</td>\n",
       "      <td>867</td>\n",
       "      <td>27469</td>\n",
       "      <td>7.97</td>\n",
       "      <td>0.19</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>774</td>\n",
       "      <td>176</td>\n",
       "      <td>1762</td>\n",
       "      <td>815</td>\n",
       "      <td>26473</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.17</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>546</td>\n",
       "      <td>122</td>\n",
       "      <td>1745</td>\n",
       "      <td>404</td>\n",
       "      <td>27183</td>\n",
       "      <td>7.69</td>\n",
       "      <td>0.12</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>542</td>\n",
       "      <td>572</td>\n",
       "      <td>1085</td>\n",
       "      <td>464</td>\n",
       "      <td>27337</td>\n",
       "      <td>7.98</td>\n",
       "      <td>0.24</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1542</td>\n",
       "      <td>229</td>\n",
       "      <td>1067</td>\n",
       "      <td>417</td>\n",
       "      <td>26745</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.37</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>675</td>\n",
       "      <td>261</td>\n",
       "      <td>1471</td>\n",
       "      <td>631</td>\n",
       "      <td>26962</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.27</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>446</td>\n",
       "      <td>277</td>\n",
       "      <td>1526</td>\n",
       "      <td>1163</td>\n",
       "      <td>26588</td>\n",
       "      <td>7.91</td>\n",
       "      <td>0.26</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177</td>\n",
       "      <td>160</td>\n",
       "      <td>1973</td>\n",
       "      <td>1951</td>\n",
       "      <td>25739</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.20</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>749</td>\n",
       "      <td>251</td>\n",
       "      <td>1716</td>\n",
       "      <td>579</td>\n",
       "      <td>26705</td>\n",
       "      <td>7.90</td>\n",
       "      <td>0.30</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>485</td>\n",
       "      <td>149</td>\n",
       "      <td>1546</td>\n",
       "      <td>652</td>\n",
       "      <td>27168</td>\n",
       "      <td>7.93</td>\n",
       "      <td>0.17</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159</td>\n",
       "      <td>522</td>\n",
       "      <td>1256</td>\n",
       "      <td>822</td>\n",
       "      <td>27241</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.34</td>\n",
       "      <td>fixed_diagmat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_diagmat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>224</td>\n",
       "      <td>174</td>\n",
       "      <td>1752</td>\n",
       "      <td>714</td>\n",
       "      <td>27136</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.28</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>761</td>\n",
       "      <td>137</td>\n",
       "      <td>1878</td>\n",
       "      <td>295</td>\n",
       "      <td>26929</td>\n",
       "      <td>7.71</td>\n",
       "      <td>0.31</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>347</td>\n",
       "      <td>325</td>\n",
       "      <td>1703</td>\n",
       "      <td>196</td>\n",
       "      <td>27429</td>\n",
       "      <td>7.82</td>\n",
       "      <td>0.24</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330</td>\n",
       "      <td>260</td>\n",
       "      <td>1370</td>\n",
       "      <td>893</td>\n",
       "      <td>27147</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.26</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>476</td>\n",
       "      <td>404</td>\n",
       "      <td>1855</td>\n",
       "      <td>514</td>\n",
       "      <td>26751</td>\n",
       "      <td>7.77</td>\n",
       "      <td>0.35</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570</td>\n",
       "      <td>295</td>\n",
       "      <td>1823</td>\n",
       "      <td>277</td>\n",
       "      <td>27035</td>\n",
       "      <td>7.75</td>\n",
       "      <td>0.30</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>741</td>\n",
       "      <td>321</td>\n",
       "      <td>1526</td>\n",
       "      <td>841</td>\n",
       "      <td>26571</td>\n",
       "      <td>7.82</td>\n",
       "      <td>0.30</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>323</td>\n",
       "      <td>181</td>\n",
       "      <td>1642</td>\n",
       "      <td>672</td>\n",
       "      <td>27182</td>\n",
       "      <td>7.89</td>\n",
       "      <td>0.21</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>690</td>\n",
       "      <td>168</td>\n",
       "      <td>1385</td>\n",
       "      <td>1450</td>\n",
       "      <td>26307</td>\n",
       "      <td>7.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>200</td>\n",
       "      <td>1009</td>\n",
       "      <td>967</td>\n",
       "      <td>27424</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.30</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291</td>\n",
       "      <td>335</td>\n",
       "      <td>1584</td>\n",
       "      <td>426</td>\n",
       "      <td>27364</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.26</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>312</td>\n",
       "      <td>352</td>\n",
       "      <td>2155</td>\n",
       "      <td>896</td>\n",
       "      <td>26285</td>\n",
       "      <td>7.63</td>\n",
       "      <td>0.21</td>\n",
       "      <td>fixed_fullMat</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>fixed_fullMat_0_R=0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arm_n_0  arm_n_1  arm_n_2  arm_n_3  arm_n_4  rew_mean  rew_std               sigmultiplier  trialnum    R scale_down_err_var knownfeatures                          unq_series\n",
       "0      418      142     1725      236    27479      7.70     0.30   scldwn_errvar_0.1_diagmat         1  0.5                0.1             0   scldwn_errvar_0.1_diagmat_0_R=0.5\n",
       "0      375      288     2132      348    26857      7.93     0.33   scldwn_errvar_0.1_diagmat         2  0.5                0.1             0   scldwn_errvar_0.1_diagmat_0_R=0.5\n",
       "0      254      318     1789      549    27090      8.00     0.29   scldwn_errvar_0.1_diagmat         3  0.5                0.1             0   scldwn_errvar_0.1_diagmat_0_R=0.5\n",
       "0      980      435     1761      941    25883      7.73     0.32   scldwn_errvar_0.1_diagmat         4  0.5                0.1             0   scldwn_errvar_0.1_diagmat_0_R=0.5\n",
       "0      389      333     1126      920    27232      7.87     0.26  scldwn_errvar_0.25_diagmat         1  0.5               0.25             0  scldwn_errvar_0.25_diagmat_0_R=0.5\n",
       "0      225      195     1165      886    27529      7.78     0.31  scldwn_errvar_0.25_diagmat         2  0.5               0.25             0  scldwn_errvar_0.25_diagmat_0_R=0.5\n",
       "0      359      340     1316     1143    26842      7.78     0.22  scldwn_errvar_0.25_diagmat         3  0.5               0.25             0  scldwn_errvar_0.25_diagmat_0_R=0.5\n",
       "0      485      189     1202     1399    26725      7.82     0.24  scldwn_errvar_0.25_diagmat         4  0.5               0.25             0  scldwn_errvar_0.25_diagmat_0_R=0.5\n",
       "0     1140      211     1813      471    26365      7.70     0.28   scldwn_errvar_0.5_diagmat         1  0.5                0.5             0   scldwn_errvar_0.5_diagmat_0_R=0.5\n",
       "0      434      114     1175     1463    26814      7.68     0.25   scldwn_errvar_0.5_diagmat         2  0.5                0.5             0   scldwn_errvar_0.5_diagmat_0_R=0.5\n",
       "0      878      112     1446     1164    26400      7.85     0.32   scldwn_errvar_0.5_diagmat         3  0.5                0.5             0   scldwn_errvar_0.5_diagmat_0_R=0.5\n",
       "0      607      345     1341      458    27249      7.74     0.26   scldwn_errvar_0.5_diagmat         4  0.5                0.5             0   scldwn_errvar_0.5_diagmat_0_R=0.5\n",
       "0     1367      593     1016      559    26465      7.79     0.33   scldwn_errvar_0.8_diagmat         1  0.5                0.8             0   scldwn_errvar_0.8_diagmat_0_R=0.5\n",
       "0      336      141     1144      955    27424      7.85     0.27   scldwn_errvar_0.8_diagmat         2  0.5                0.8             0   scldwn_errvar_0.8_diagmat_0_R=0.5\n",
       "0      312      198      847      309    28334      7.84     0.39   scldwn_errvar_0.8_diagmat         3  0.5                0.8             0   scldwn_errvar_0.8_diagmat_0_R=0.5\n",
       "0      410      163     1784      858    26785      7.77     0.25   scldwn_errvar_0.8_diagmat         4  0.5                0.8             0   scldwn_errvar_0.8_diagmat_0_R=0.5\n",
       "0      300      224     1755     1444    26277      7.72     0.28   scldwn_errvar_0.1_fullMat         1  0.5                0.1             0   scldwn_errvar_0.1_fullMat_0_R=0.5\n",
       "0      514      383     1656      310    27137      7.86     0.21   scldwn_errvar_0.1_fullMat         2  0.5                0.1             0   scldwn_errvar_0.1_fullMat_0_R=0.5\n",
       "0      567      565     1578      548    26742      7.88     0.25   scldwn_errvar_0.1_fullMat         3  0.5                0.1             0   scldwn_errvar_0.1_fullMat_0_R=0.5\n",
       "0      261      186      937     1150    27466      7.83     0.26   scldwn_errvar_0.1_fullMat         4  0.5                0.1             0   scldwn_errvar_0.1_fullMat_0_R=0.5\n",
       "0      984      462     1485      611    26458      7.98     0.34  scldwn_errvar_0.25_fullMat         1  0.5               0.25             0  scldwn_errvar_0.25_fullMat_0_R=0.5\n",
       "0     1241      137     1493     1156    25973      7.78     0.25  scldwn_errvar_0.25_fullMat         2  0.5               0.25             0  scldwn_errvar_0.25_fullMat_0_R=0.5\n",
       "0      446      257     1462     1115    26720      7.83     0.30  scldwn_errvar_0.25_fullMat         3  0.5               0.25             0  scldwn_errvar_0.25_fullMat_0_R=0.5\n",
       "0      446      119     1809     1182    26444      7.86     0.27  scldwn_errvar_0.25_fullMat         4  0.5               0.25             0  scldwn_errvar_0.25_fullMat_0_R=0.5\n",
       "0      589      386     1976      697    26352      7.84     0.23   scldwn_errvar_0.5_fullMat         1  0.5                0.5             0   scldwn_errvar_0.5_fullMat_0_R=0.5\n",
       "0      723      129      944     1082    27122      7.74     0.27   scldwn_errvar_0.5_fullMat         2  0.5                0.5             0   scldwn_errvar_0.5_fullMat_0_R=0.5\n",
       "0      308      296     1600     1014    26782      7.86     0.18   scldwn_errvar_0.5_fullMat         3  0.5                0.5             0   scldwn_errvar_0.5_fullMat_0_R=0.5\n",
       "0      872      257     1347      592    26932      7.77     0.31   scldwn_errvar_0.5_fullMat         4  0.5                0.5             0   scldwn_errvar_0.5_fullMat_0_R=0.5\n",
       "0      734      217     1642      511    26896      7.89     0.36   scldwn_errvar_0.8_fullMat         1  0.5                0.8             0   scldwn_errvar_0.8_fullMat_0_R=0.5\n",
       "0     1104      131     1596      928    26241      7.91     0.30   scldwn_errvar_0.8_fullMat         2  0.5                0.8             0   scldwn_errvar_0.8_fullMat_0_R=0.5\n",
       "0      927      129     1108      694    27142      7.71     0.23   scldwn_errvar_0.8_fullMat         3  0.5                0.8             0   scldwn_errvar_0.8_fullMat_0_R=0.5\n",
       "0      654      481      821      809    27235      7.80     0.24   scldwn_errvar_0.8_fullMat         4  0.5                0.8             0   scldwn_errvar_0.8_fullMat_0_R=0.5\n",
       "0      314      291     1512      944    26939      7.82     0.25               fixed_diagmat         1  0.1                0.8             0               fixed_diagmat_0_R=0.1\n",
       "0      329      173     1162      867    27469      7.97     0.19               fixed_diagmat         2  0.1                0.8             0               fixed_diagmat_0_R=0.1\n",
       "0      774      176     1762      815    26473      7.90     0.17               fixed_diagmat         3  0.1                0.8             0               fixed_diagmat_0_R=0.1\n",
       "0      546      122     1745      404    27183      7.69     0.12               fixed_diagmat         4  0.1                0.8             0               fixed_diagmat_0_R=0.1\n",
       "0      542      572     1085      464    27337      7.98     0.24               fixed_diagmat         1  0.2                0.8             0               fixed_diagmat_0_R=0.2\n",
       "0     1542      229     1067      417    26745      7.81     0.37               fixed_diagmat         2  0.2                0.8             0               fixed_diagmat_0_R=0.2\n",
       "0      675      261     1471      631    26962      7.81     0.27               fixed_diagmat         3  0.2                0.8             0               fixed_diagmat_0_R=0.2\n",
       "0      446      277     1526     1163    26588      7.91     0.26               fixed_diagmat         4  0.2                0.8             0               fixed_diagmat_0_R=0.2\n",
       "0      177      160     1973     1951    25739      7.77     0.20               fixed_diagmat         1  0.5                0.8             0               fixed_diagmat_0_R=0.5\n",
       "0      749      251     1716      579    26705      7.90     0.30               fixed_diagmat         2  0.5                0.8             0               fixed_diagmat_0_R=0.5\n",
       "0      485      149     1546      652    27168      7.93     0.17               fixed_diagmat         3  0.5                0.8             0               fixed_diagmat_0_R=0.5\n",
       "0      159      522     1256      822    27241      7.73     0.34               fixed_diagmat         4  0.5                0.8             0               fixed_diagmat_0_R=0.5\n",
       "0      224      174     1752      714    27136      7.78     0.28               fixed_fullMat         1  0.1                0.8             0               fixed_fullMat_0_R=0.1\n",
       "0      761      137     1878      295    26929      7.71     0.31               fixed_fullMat         2  0.1                0.8             0               fixed_fullMat_0_R=0.1\n",
       "0      347      325     1703      196    27429      7.82     0.24               fixed_fullMat         3  0.1                0.8             0               fixed_fullMat_0_R=0.1\n",
       "0      330      260     1370      893    27147      7.86     0.26               fixed_fullMat         4  0.1                0.8             0               fixed_fullMat_0_R=0.1\n",
       "0      476      404     1855      514    26751      7.77     0.35               fixed_fullMat         1  0.2                0.8             0               fixed_fullMat_0_R=0.2\n",
       "0      570      295     1823      277    27035      7.75     0.30               fixed_fullMat         2  0.2                0.8             0               fixed_fullMat_0_R=0.2\n",
       "0      741      321     1526      841    26571      7.82     0.30               fixed_fullMat         3  0.2                0.8             0               fixed_fullMat_0_R=0.2\n",
       "0      323      181     1642      672    27182      7.89     0.21               fixed_fullMat         4  0.2                0.8             0               fixed_fullMat_0_R=0.2\n",
       "0      690      168     1385     1450    26307      7.73     0.20               fixed_fullMat         1  0.5                0.8             0               fixed_fullMat_0_R=0.5\n",
       "0      400      200     1009      967    27424      7.78     0.30               fixed_fullMat         2  0.5                0.8             0               fixed_fullMat_0_R=0.5\n",
       "0      291      335     1584      426    27364      7.83     0.26               fixed_fullMat         3  0.5                0.8             0               fixed_fullMat_0_R=0.5\n",
       "0      312      352     2155      896    26285      7.63     0.21               fixed_fullMat         4  0.5                0.8             0               fixed_fullMat_0_R=0.5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults['unq_series'] = allresults['sigmultiplier'] + '_'+ allresults['knownfeatures'] + '_R='+ allresults['R'].astype(str)\n",
    "allresults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scldwn_errvar_0.1_diagmat_0_R=0.5',\n",
       " 'scldwn_errvar_0.25_diagmat_0_R=0.5',\n",
       " 'scldwn_errvar_0.5_diagmat_0_R=0.5',\n",
       " 'scldwn_errvar_0.8_diagmat_0_R=0.5',\n",
       " 'scldwn_errvar_0.1_fullMat_0_R=0.5',\n",
       " 'scldwn_errvar_0.25_fullMat_0_R=0.5',\n",
       " 'scldwn_errvar_0.5_fullMat_0_R=0.5',\n",
       " 'scldwn_errvar_0.8_fullMat_0_R=0.5',\n",
       " 'fixed_diagmat_0_R=0.1',\n",
       " 'fixed_diagmat_0_R=0.2',\n",
       " 'fixed_diagmat_0_R=0.5',\n",
       " 'fixed_fullMat_0_R=0.1',\n",
       " 'fixed_fullMat_0_R=0.2',\n",
       " 'fixed_fullMat_0_R=0.5']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allresults.unq_series.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(25, 15))\n",
    "# sns.lineplot(data=allresults, x='trialnum', y='rew_mean', hue='unq_series', style='unq_series', ax=ax, markers = True, ci=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unq_series_</th>\n",
       "      <th>rew_mean_mean</th>\n",
       "      <th>rew_mean_median</th>\n",
       "      <th>rew_mean_std</th>\n",
       "      <th>rew_std_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fixed_diagmat_0_R=0.2</td>\n",
       "      <td>7.88</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>scldwn_errvar_0.25_fullMat_0_R=0.5</td>\n",
       "      <td>7.86</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fixed_diagmat_0_R=0.1</td>\n",
       "      <td>7.85</td>\n",
       "      <td>7.86</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>scldwn_errvar_0.1_diagmat_0_R=0.5</td>\n",
       "      <td>7.84</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fixed_diagmat_0_R=0.5</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>scldwn_errvar_0.8_fullMat_0_R=0.5</td>\n",
       "      <td>7.83</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>scldwn_errvar_0.1_fullMat_0_R=0.5</td>\n",
       "      <td>7.82</td>\n",
       "      <td>7.84</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>scldwn_errvar_0.8_diagmat_0_R=0.5</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.81</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>scldwn_errvar_0.25_diagmat_0_R=0.5</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fixed_fullMat_0_R=0.2</td>\n",
       "      <td>7.81</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>scldwn_errvar_0.5_fullMat_0_R=0.5</td>\n",
       "      <td>7.80</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fixed_fullMat_0_R=0.1</td>\n",
       "      <td>7.79</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fixed_fullMat_0_R=0.5</td>\n",
       "      <td>7.74</td>\n",
       "      <td>7.76</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>scldwn_errvar_0.5_diagmat_0_R=0.5</td>\n",
       "      <td>7.74</td>\n",
       "      <td>7.72</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           unq_series_  rew_mean_mean  rew_mean_median  rew_mean_std  rew_std_mean\n",
       "1                fixed_diagmat_0_R=0.2           7.88             7.86          0.08          0.29\n",
       "9   scldwn_errvar_0.25_fullMat_0_R=0.5           7.86             7.84          0.09          0.29\n",
       "0                fixed_diagmat_0_R=0.1           7.85             7.86          0.12          0.18\n",
       "6    scldwn_errvar_0.1_diagmat_0_R=0.5           7.84             7.83          0.15          0.31\n",
       "2                fixed_diagmat_0_R=0.5           7.83             7.84          0.10          0.25\n",
       "13   scldwn_errvar_0.8_fullMat_0_R=0.5           7.83             7.84          0.09          0.28\n",
       "7    scldwn_errvar_0.1_fullMat_0_R=0.5           7.82             7.84          0.07          0.25\n",
       "12   scldwn_errvar_0.8_diagmat_0_R=0.5           7.81             7.81          0.04          0.31\n",
       "8   scldwn_errvar_0.25_diagmat_0_R=0.5           7.81             7.80          0.04          0.25\n",
       "4                fixed_fullMat_0_R=0.2           7.81             7.79          0.06          0.29\n",
       "11   scldwn_errvar_0.5_fullMat_0_R=0.5           7.80             7.80          0.06          0.25\n",
       "3                fixed_fullMat_0_R=0.1           7.79             7.80          0.06          0.27\n",
       "5                fixed_fullMat_0_R=0.5           7.74             7.76          0.09          0.24\n",
       "10   scldwn_errvar_0.5_diagmat_0_R=0.5           7.74             7.72          0.07          0.28"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfagg = allresults.groupby('unq_series').agg({'rew_mean':['mean', 'median', 'std'], 'rew_std':['mean']}).reset_index()\n",
    "dfagg.columns = [tpl[0] + '_' + tpl[1] for tpl in dfagg.columns]\n",
    "dfagg.sort_values(by='rew_mean_mean', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p36",
   "language": "python",
   "name": "pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
